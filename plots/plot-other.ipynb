{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, roc_curve, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = Path(\"/opt/gpudata/labrag\")\n",
    "\n",
    "dataset_models = [(\"mimic\", \"biovilt\"), (\"chexpertplus\", \"gloria\")]\n",
    "sections = [\"findings\", \"impression\"]\n",
    "labelers = [\"chexbert\", \"chexpert\"]\n",
    "\n",
    "labels = [\n",
    "    \"Atelectasis\",\n",
    "    \"Cardiomegaly\",\n",
    "    \"Consolidation\",\n",
    "    \"Edema\",\n",
    "    \"Enlarged Cardiomediastinum\",\n",
    "    \"Fracture\",\n",
    "    \"Lung Lesion\",\n",
    "    \"Lung Opacity\",\n",
    "    \"No Finding\",\n",
    "    \"Pleural Effusion\",\n",
    "    \"Pleural Other\",\n",
    "    \"Pneumonia\",\n",
    "    \"Pneumothorax\",\n",
    "    \"Support Devices\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "for dataset, base_model in dataset_models:\n",
    "    for section in sections:\n",
    "        for model in [base_model, \"resnet50\"]:\n",
    "            for labeler in labelers:\n",
    "                cls_dir = exp_dir / f\"{dataset}-{section}-{model}-classifiers-{labeler}\"\n",
    "                df_true = pd.read_csv(cls_dir / \"test_true.csv\")\n",
    "                df_prob = pd.read_csv(cls_dir / \"test_prob.csv\")\n",
    "                df_pred = pd.read_csv(cls_dir / \"pred_pr.csv\")\n",
    "                df_pred = df_pred.set_index(\"study_id\").loc[df_true[\"study_id\"]].reset_index()\n",
    "                for label in labels:\n",
    "                    fpr, tpr, _ = roc_curve(df_true[label], df_prob[label])\n",
    "                    auroc = auc(fpr, tpr)\n",
    "                    precision, recall, _ = precision_recall_curve(df_true[label], df_prob[label])\n",
    "                    auprc = auc(recall, precision)\n",
    "                    f1 = f1_score(df_true[label], df_pred[label])\n",
    "                    results.append({\n",
    "                        \"dataset\": dataset,\n",
    "                        \"section\": section,\n",
    "                        \"model\": model,\n",
    "                        \"labeler\": labeler,\n",
    "                        \"label\": label,\n",
    "                        \"auroc\": auroc,\n",
    "                        \"auprc\": auprc,\n",
    "                        \"f1\": f1,\n",
    "                    })\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_results = results.set_index([\"model\", \"labeler\", \"dataset\", \"section\"])[[\"label\", \"f1\"]].pivot(columns=\"label\", values=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_results.columns.name = None\n",
    "f1_results.index.names = [n.title() for n in f1_results.index.names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_results.to_latex(float_format=\"%.2f\").replace(\"Enlarged Cardiomediastinum\", \"Enl. Card.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Filtered Similarity Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sys.path.append(\"../rrg\")\n",
    "from _data import get_per_study_data, get_split_features, get_split_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_root = Path(\"/opt/gpudata/chexpertplus\")\n",
    "mimic_root = Path(\"/opt/gpudata/mimic-cxr\")\n",
    "exp_dir = Path(\"/opt/gpudata/labrag\")\n",
    "label_dir = Path(\"/opt/gpudata/cxr-derived\")\n",
    "\n",
    "img_key = \"img_proj\"\n",
    "datasets = [\"mimic\"]\n",
    "sections = [\"findings\", \"impression\"]\n",
    "fs = [\"exact\", \"partial\"]\n",
    "\n",
    "dataset_2_model = {\n",
    "    \"chexpertplus\": \"gloria\",\n",
    "    \"mimic\": \"biovilt\",\n",
    "}\n",
    "dataset_2_csvs = {\n",
    "    \"chexpertplus\": (\n",
    "        chexpert_root / \"split.csv\",\n",
    "        chexpert_root / \"metadata.csv\",\n",
    "        chexpert_root / \"report.csv\",\n",
    "    ),\n",
    "    \"mimic\": (\n",
    "        mimic_root / \"mimic-cxr-2.0.0-split.csv\",\n",
    "        mimic_root / \"mimic-cxr-2.0.0-metadata.csv\",\n",
    "        mimic_root / \"mimic_cxr_sectioned.csv\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "split_remap = {\n",
    "    \"train\": \"retrieval\",\n",
    "    \"validate\": \"retrieval\",\n",
    "    \"test\": \"inference\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = defaultdict(lambda: defaultdict(dict))\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    h5_path = exp_dir / f\"{dataset}-{dataset_2_model[dataset]}.h5\"\n",
    "    split_csv, metadata_csv, report_csv = dataset_2_csvs[dataset]\n",
    "    for section in sections:\n",
    "        print(section)\n",
    "        all_df = get_per_study_data(\n",
    "            split_csv=split_csv,\n",
    "            metadata_csv=metadata_csv,\n",
    "            label_csv=label_dir / f\"{dataset}-{section}-labels-chexbert.csv\",\n",
    "            report_csv=report_csv,\n",
    "            split_remap=split_remap,\n",
    "        )\n",
    "        split_samples = get_split_samples(sample_df=all_df)\n",
    "        features = get_split_features(\n",
    "            feature_h5=h5_path,\n",
    "            feature_key=img_key,\n",
    "            sample_df=all_df,\n",
    "        )\n",
    "\n",
    "        retrieval_df = split_samples[\"retrieval\"]\n",
    "        inference_df = split_samples[\"inference\"]\n",
    "        retrieval_features = features[\"retrieval\"]\n",
    "        inference_features = features[\"inference\"]\n",
    "        retrieval_df[\"study_id\"] = retrieval_df[\"study_id\"].astype(str)\n",
    "        inference_df[\"study_id\"] = inference_df[\"study_id\"].astype(str)\n",
    "        for f in fs:\n",
    "            print(f)\n",
    "            res = pd.read_csv(exp_dir / f\"exp-{dataset}\" / f\"exp-{section}\" / \"exp-filter\" / f\"{section}_top-5_{dataset_2_model[dataset]}-chexbert-pr-pred-label_{f}_simple_Mistral-7B-Instruct-v0.3.csv\")\n",
    "            res[\"temp\"] = res[\"retrieved_studies\"].str.split(\", \")\n",
    "            df = res[[\"study_id\", \"temp\"]].explode(\"temp\")\n",
    "            df[\"study_id\"] = df[\"study_id\"].astype(str)\n",
    "            df = df.dropna()\n",
    "\n",
    "            inference_idxs = inference_df.reset_index().set_index(\"study_id\").loc[df[\"study_id\"], \"index\"].to_numpy()\n",
    "            retrieval_idxs = retrieval_df.reset_index().set_index(\"study_id\").loc[df[\"temp\"], \"index\"].to_numpy()\n",
    "\n",
    "            sims = cosine_similarity(inference_features, retrieval_features)\n",
    "            sort_idxs = sims.argsort(axis=1)\n",
    "            ranks = sort_idxs.argsort(axis=1)\n",
    "\n",
    "            retrieved_ranks = (len(retrieval_features)-1) - ranks[inference_idxs, retrieval_idxs]\n",
    "            df[\"retrieved_rank\"] = retrieved_ranks\n",
    "            dfs[dataset][section][f] = (df, res[\"study_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap = sns.color_palette(palette=\"Set3\")\n",
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(6, 3))\n",
    "\n",
    "bins = list(range(27))\n",
    "bins[-1] = 1000000\n",
    "\n",
    "findings_exact, findings_n = dfs[\"mimic\"][\"findings\"][\"exact\"]\n",
    "findings_partial, _ = dfs[\"mimic\"][\"findings\"][\"partial\"]\n",
    "\n",
    "impression_exact, impression_n = dfs[\"mimic\"][\"impression\"][\"exact\"]\n",
    "impression_partial, _ = dfs[\"mimic\"][\"impression\"][\"partial\"]\n",
    "\n",
    "sns.histplot(findings_exact[\"retrieved_rank\"], bins=bins, ax=ax1, color=cmap[4], linewidth=1, zorder=10, alpha=1, edgecolor=\"grey\")\n",
    "sns.histplot(findings_partial[\"retrieved_rank\"], bins=bins, ax=ax3, color=cmap[1], linewidth=1, zorder=10, alpha=1, edgecolor=\"grey\")\n",
    "\n",
    "sns.histplot(impression_exact[\"retrieved_rank\"], bins=bins, ax=ax2, color=cmap[4], linewidth=1, zorder=10, alpha=1, edgecolor=\"grey\")\n",
    "sns.histplot(impression_partial[\"retrieved_rank\"], bins=bins, ax=ax4, color=cmap[1], linewidth=1, zorder=10, alpha=1, edgecolor=\"grey\")\n",
    "\n",
    "ax1.set_title(f\"Exact Filter, Findings, N={findings_n}\", fontsize=10)\n",
    "ax3.set_title(f\"Partial Filter, Findings, N={findings_n}\", fontsize=10)\n",
    "\n",
    "ax2.set_title(f\"Exact Filter, Impression, N={impression_n}\", fontsize=10)\n",
    "ax4.set_title(f\"Partial Filter, Impression, N={impression_n}\", fontsize=10)\n",
    "\n",
    "ax1.set_ylim([0, 550])\n",
    "ax2.set_ylim([0, 550])\n",
    "ax3.set_ylim([0, 550])\n",
    "ax4.set_ylim([0, 550])\n",
    "\n",
    "ax1.set_xlim([0, 25])\n",
    "ax2.set_xlim([0, 25])\n",
    "ax3.set_xlim([0, 25])\n",
    "ax4.set_xlim([0, 25])\n",
    "\n",
    "ax1.set_xticks([0, 5, 10, 15, 20, 25])\n",
    "ax2.set_xticks([0, 5, 10, 15, 20, 25])\n",
    "ax3.set_xticks([0, 5, 10, 15, 20, 25])\n",
    "ax4.set_xticks([0, 5, 10, 15, 20, 25])\n",
    "\n",
    "ax1.set_xticklabels([])\n",
    "ax3.set_xticklabels([])\n",
    "\n",
    "ax2.set_xticklabels([0, 5, 10, 15, 20, 25])\n",
    "ax4.set_xticklabels([0, 5, 10, 15, 20, 25])\n",
    "\n",
    "ax1.set_xlabel(\"\")\n",
    "ax2.set_xlabel(\"\")\n",
    "ax3.set_xlabel(\"Image Similarity Rank\")\n",
    "ax4.set_xlabel(\"Image Similarity Rank\")\n",
    "\n",
    "ax2.set_ylabel(\"\")\n",
    "ax4.set_ylabel(\"\")\n",
    "\n",
    "ax1.set_yticks([0, 100, 200, 300, 400, 500])\n",
    "ax2.set_yticks([0, 100, 200, 300, 400, 500])\n",
    "ax3.set_yticks([0, 100, 200, 300, 400, 500])\n",
    "ax4.set_yticks([0, 100, 200, 300, 400, 500])\n",
    "\n",
    "ax1.set_yticklabels([0, 100, 200, 300, 400, 500])\n",
    "ax2.set_yticklabels([])\n",
    "ax3.set_yticklabels([0, 100, 200, 300, 400, 500])\n",
    "ax4.set_yticklabels([])\n",
    "\n",
    "ax1.grid(which=\"major\", axis=\"y\", zorder=0)\n",
    "ax2.grid(which=\"major\", axis=\"y\", zorder=0)\n",
    "ax3.grid(which=\"major\", axis=\"y\", zorder=0)\n",
    "ax4.grid(which=\"major\", axis=\"y\", zorder=0)\n",
    "\n",
    "for ax in [ax1, ax2, ax3, ax4]:\n",
    "    ax.spines[\"bottom\"].set_zorder(20)\n",
    "    ax.spines[\"left\"].set_zorder(20)\n",
    "\n",
    "fig.suptitle(\"Top 5 Filtered Image Similarity\", y=.95, fontsize=10)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(f\"../figs/other/selected-similarity-rank.png\", dpi=1000)\n",
    "fig.savefig(f\"../figs/other/selected-similarity-rank.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from f1chexbert import F1CheXbert\n",
    "\n",
    "m = F1CheXbert(device=\"cpu\")\n",
    "\n",
    "def get_radgraph_tokens(radgraph_blob: str) -> list[str]:\n",
    "    dat = eval(radgraph_blob) # here be dragons\n",
    "    toks = []\n",
    "    for _, x in dat[\"entities\"].items():\n",
    "        toks.append(x[\"tokens\"])\n",
    "    return toks\n",
    "\n",
    "def get_chexbert_names(pos_labels: str | list[int]) -> list[str]:\n",
    "    if isinstance(pos_labels, str):\n",
    "        pos_labels = eval(pos_labels) # more dragons\n",
    "    pos_labels = np.asarray(pos_labels)\n",
    "    return np.asarray(m.target_names)[pos_labels == 1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = Path(\"/opt/gpudata/labrag\")\n",
    "\n",
    "scores = pd.read_csv(exp_dir / \"exp-mimic/exp-findings/exp-core/findings_top-5_biovilt-chexbert-pr-pred-label_exact_simple_Mistral-7B-Instruct-v0.3_METRICS.csv\")\n",
    "generations = pd.read_csv(exp_dir / \"exp-mimic/exp-findings/exp-core/findings_top-5_biovilt-chexbert-pr-pred-label_exact_simple_Mistral-7B-Instruct-v0.3.csv\")\n",
    "\n",
    "cxrmate_scores = pd.read_csv(exp_dir / \"exp-baselines/cxrmate_findings_METRICS.csv\")\n",
    "cxrmate_text = pd.read_csv(exp_dir / \"exp-baselines/cxrmate_findings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = \"AP view of the chest.  Right PICC is seen with tip at the upper SVC.\"\n",
    "mask = generations[\"actual_text\"].str.contains(temp)\n",
    "assert mask.sum() == 1\n",
    "\n",
    "idx = generations.index[mask][0]\n",
    "study_id = generations.loc[idx, \"study_id\"]\n",
    "print(generations.loc[idx, \"actual_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chexbert_names(scores.loc[idx, \"actual_chexbert\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chexbert_names(m.get_label(\"PICC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chexbert_names(m.get_label(\"Cardiac silhouette appears moderately enlarged\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_radgraph_tokens(scores.loc[idx, \"actual_radgraph\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LaB-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc[idx, [\"f1radgraph\", \"f1chexbert\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generations.loc[idx, \"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chexbert_names(scores.loc[idx, \"generated_chexbert\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chexbert_names(m.get_label(\"internal jugular catheter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chexbert_names(m.get_label(\"cardiomegaly\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_radgraph_tokens(scores.loc[idx, \"generated_radgraph\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CXRMate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxrmate_mask = cxrmate_scores[\"study_id\"] == study_id\n",
    "assert cxrmate_mask.sum() == 1\n",
    "cxrmate_idx = cxrmate_scores.index[mask][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxrmate_scores.loc[cxrmate_idx, [\"f1radgraph\", \"f1chexbert\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(cxrmate_text.loc[cxrmate_idx, \"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chexbert_names(cxrmate_scores.loc[cxrmate_idx, \"generated_chexbert\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chexbert_names(m.get_label(\"PICC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chexbert_names(m.get_label(\"cardiac silhouette is enlarged\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_radgraph_tokens(cxrmate_scores.loc[cxrmate_idx, \"generated_radgraph\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tableone import TableOne\n",
    "\n",
    "sys.path.append(\"../rrg\")\n",
    "from _data import get_per_study_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_root = Path(\"/opt/gpudata/chexpertplus\")\n",
    "mimic_root = Path(\"/opt/gpudata/mimic-cxr\")\n",
    "exp_dir = Path(\"/opt/gpudata/labrag\")\n",
    "label_dir = Path(\"/opt/gpudata/cxr-derived\")\n",
    "\n",
    "datasets = [\"chexpertplus\", \"mimic\"]\n",
    "sections = [\"findings\", \"impression\"]\n",
    "\n",
    "dataset_2_csvs = {\n",
    "    \"chexpertplus\": (\n",
    "        chexpert_root / \"split.csv\",\n",
    "        chexpert_root / \"metadata.csv\",\n",
    "        chexpert_root / \"report.csv\",\n",
    "    ),\n",
    "    \"mimic\": (\n",
    "        mimic_root / \"mimic-cxr-2.0.0-split.csv\",\n",
    "        mimic_root / \"mimic-cxr-2.0.0-metadata.csv\",\n",
    "        mimic_root / \"mimic_cxr_sectioned.csv\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = defaultdict(dict)\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    split_csv, metadata_csv, report_csv = dataset_2_csvs[dataset]\n",
    "    meta = pd.read_csv(metadata_csv)\n",
    "    for section in sections:\n",
    "        print(section)\n",
    "        df = get_per_study_data(\n",
    "            split_csv=split_csv,\n",
    "            metadata_csv=metadata_csv,\n",
    "            label_csv=label_dir / f\"{dataset}-{section}-labels-chexbert.csv\",\n",
    "            report_csv=report_csv,\n",
    "            extra_meta_cols=[\"StudyDate\"] if dataset == \"mimic\" else [\"age\", \"sex\", \"race\", \"ethnicity\"],\n",
    "        )\n",
    "\n",
    "        dfs[dataset][section] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1 Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table_one(demo: pd.DataFrame) -> pd.DataFrame:\n",
    "    tab1 = TableOne(\n",
    "        data=demo,\n",
    "        columns=[\"Age\", \"Sex\", \"Race\"],\n",
    "        categorical=[\"Sex\", \"Race\"],\n",
    "        continuous=[\"Age\"],\n",
    "        groupby=\"split\",\n",
    "        nonnormal=[\"Age\"],\n",
    "        order={\n",
    "            \"Sex\": [\"Female\", \"Male\", \"Unknown\"],\n",
    "            \"Race\": [\"White\", \"Black\", \"Hispanic/Latino\", \"Asian\", \"AIAN\", \"NHPI\", \"Other\", \"Unknown\"],\n",
    "        }\n",
    "    ).tableone\n",
    "\n",
    "    tab1.columns = tab1.columns.get_level_values(1)\n",
    "    tab1 = tab1[[\"Overall\", \"train\", \"validate\", \"test\"]]\n",
    "\n",
    "    overall_patients = demo[\"subject_id\"].nunique()\n",
    "    split_patients = demo.groupby(\"split\")[\"subject_id\"].nunique()\n",
    "    split_patients.loc[\"Overall\"] = overall_patients\n",
    "    tab1.loc[(\"Count, N\", \"Patients\"), :] = split_patients\n",
    "\n",
    "    overall_size = len(demo)\n",
    "    split_sizes = demo.groupby(\"split\").size()\n",
    "    split_sizes.loc[\"Overall\"] = overall_size\n",
    "\n",
    "    overall_missing_age = demo[\"Age\"].isna().sum()\n",
    "    split_missing_age = demo.loc[demo[\"Age\"].isna()].groupby(\"split\").size()\n",
    "    split_missing_age.loc[\"Overall\"] = overall_missing_age\n",
    "\n",
    "    split_missing_age_percent = (split_missing_age / split_sizes * 100).apply(lambda x: f\"{x:0.1f}\")\n",
    "    split_missing_age = split_missing_age.astype(str) + \" (\" + split_missing_age_percent + \")\"\n",
    "\n",
    "    tab1.loc[(\"Age\", \"Missing, N (%)\"), :] = split_missing_age\n",
    "\n",
    "    tab1 = tab1.fillna(\"\")\n",
    "\n",
    "    tab1.columns = tab1.columns.str.title()\n",
    "\n",
    "    tab1.index = pd.MultiIndex.from_tuples([\n",
    "        (\"Count, N\", \"Studies\"),\n",
    "        (\"Age\", \"Median [Q1, Q3]\"),\n",
    "        (\"Sex, N (%)\", \"Female\"),\n",
    "        (\"Sex, N (%)\", \"Male\"),\n",
    "        (\"Sex, N (%)\", \"Unknown\"),\n",
    "        (\"Race, N (%)\", \"White\"),\n",
    "        (\"Race, N (%)\", \"Black\"),\n",
    "        (\"Race, N (%)\", \"Hispanic/Latino\"),\n",
    "        (\"Race, N (%)\", \"Asian\"),\n",
    "        (\"Race, N (%)\", \"AIAN\"),\n",
    "        (\"Race, N (%)\", \"NHPI\"),\n",
    "        (\"Race, N (%)\", \"Other\"),\n",
    "        (\"Race, N (%)\", \"Unknown\"),\n",
    "        (\"Count, N\", \"Patients\"),\n",
    "        (\"Age\", \"Missing, N (%)\"),\n",
    "    ])\n",
    "\n",
    "    tab1 = tab1.loc[[\"Count, N\", \"Age\", \"Sex, N (%)\", \"Race, N (%)\"]]\n",
    "    return tab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Table Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_view_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    view_tab = TableOne(\n",
    "        data=df.replace({\"ViewPosition\": {\"\": \"Unknown\"}}),\n",
    "        columns=[\"ViewPosition\"],\n",
    "        categorical=[\"ViewPosition\"],\n",
    "        groupby=\"split\",\n",
    "        order={\n",
    "            \"ViewPosition\": [\n",
    "                \"PA\",\n",
    "                \"AP\",\n",
    "                \"LATERAL\",\n",
    "                \"LL\",\n",
    "                \"AP AXIAL\",\n",
    "                \"AP LLD\",\n",
    "                \"AP RLD\",\n",
    "                \"PA RLD\",\n",
    "                \"PA LLD\",\n",
    "                \"LAO\",\n",
    "                \"RAO\",\n",
    "                \"LPO\",\n",
    "                \"XTABLE LATERAL\",\n",
    "                \"SWIMMERS\",\n",
    "                \"Unknown\",\n",
    "            ],\n",
    "        }\n",
    "    ).tableone\n",
    "\n",
    "    view_tab.columns = [x.title() for x in view_tab.columns.get_level_values(1)]\n",
    "    view_tab = view_tab[[\"Overall\", \"Train\", \"Validate\", \"Test\"]]\n",
    "    views = view_tab.index.get_level_values(1)[1:].to_list()\n",
    "    view_tab.index = [\"Total\"] + views\n",
    "    view_tab = view_tab.loc[views + [\"Total\"]]\n",
    "    view_tab.index.name = \"View, N (%)\"\n",
    "    view_tab = view_tab.reset_index()\n",
    "    return view_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Prevalence Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"Atelectasis\",\n",
    "    \"Cardiomegaly\",\n",
    "    \"Consolidation\",\n",
    "    \"Edema\",\n",
    "    \"Enlarged Cardiomediastinum\",\n",
    "    \"Fracture\",\n",
    "    \"Lung Lesion\",\n",
    "    \"Lung Opacity\",\n",
    "    \"No Finding\",\n",
    "    \"Pleural Effusion\",\n",
    "    \"Pleural Other\",\n",
    "    \"Pneumonia\",\n",
    "    \"Pneumothorax\",\n",
    "    \"Support Devices\",\n",
    "]\n",
    "\n",
    "def make_label_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    label_tab = TableOne(\n",
    "        data=df,\n",
    "        columns=labels,\n",
    "        categorical=labels,\n",
    "        groupby=\"split\",\n",
    "    ).tableone\n",
    "\n",
    "    label_tab.columns = [x.title() for x in label_tab.columns.get_level_values(1)]\n",
    "    label_tab = label_tab[[\"Overall\", \"Train\", \"Validate\", \"Test\"]]\n",
    "    label_tab = label_tab.loc[label_tab.index.get_level_values(1).isin([\"\", \"1\"])].copy()\n",
    "    label_tab.index = [\"Count, N\"] + [f\"{l}, N (%)\" for l in labels]\n",
    "    return label_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC\n",
    "requires joining back to MIMIC-IV metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admits = pd.read_csv(\"/opt/gpudata/mimic/iv/hosp/admissions.csv.gz\")\n",
    "race = admits.sort_values(\"admittime\").drop_duplicates(\"subject_id\", keep=\"last\")[[\"subject_id\", \"race\"]]\n",
    "race = race.set_index(\"subject_id\")[\"race\"]\n",
    "\n",
    "patients = pd.read_csv(\"/opt/gpudata/mimic/iv/hosp/patients.csv.gz\")\n",
    "assert not patients[\"subject_id\"].duplicated().any()\n",
    "\n",
    "patients[\"race\"] = patients[\"subject_id\"].apply(lambda sid: race.get(sid, np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mimic_demo(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"StudyYear\"] = pd.to_datetime(df[\"StudyDate\"], format=\"%Y%m%d\").dt.year\n",
    "\n",
    "    demo = df.merge(patients, on=\"subject_id\", how=\"left\")[\n",
    "        [\n",
    "            \"subject_id\",\n",
    "            \"study_id\",\n",
    "            \"dicom_id\",\n",
    "            \"split\",\n",
    "            \"ViewPosition\",\n",
    "            \"StudyYear\",\n",
    "            \"gender\",\n",
    "            \"anchor_age\",\n",
    "            \"anchor_year\",\n",
    "            \"race\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    demo[\"Age\"] = (demo[\"anchor_age\"] + (demo[\"StudyYear\"] - demo[\"anchor_year\"]))\n",
    "    demo[\"Sex\"] = demo[\"gender\"].replace({\"M\": \"Male\", \"F\": \"Female\"}).fillna(\"Unknown\")\n",
    "    demo[\"Race\"] = demo[\"race\"].replace({\n",
    "        \"WHITE\": \"White\",\n",
    "        \"BLACK/AFRICAN AMERICAN\": \"Black\",\n",
    "        \"UNKNOWN\": \"Unknown\",\n",
    "        \"WHITE - OTHER EUROPEAN\": \"White\",\n",
    "        \"OTHER\": \"Other\",\n",
    "        \"HISPANIC/LATINO - PUERTO RICAN\": \"Hispanic/Latino\",\n",
    "        \"WHITE - RUSSIAN\": \"White\",\n",
    "        \"ASIAN - CHINESE\": \"Asian\",\n",
    "        \"BLACK/CAPE VERDEAN\": \"Black\",\n",
    "        \"HISPANIC/LATINO - DOMINICAN\": \"Hispanic/Latino\",\n",
    "        \"ASIAN\": \"Asian\",\n",
    "        \"BLACK/CARIBBEAN ISLAND\": \"Black\",\n",
    "        \"BLACK/AFRICAN\": \"Black\",\n",
    "        \"PORTUGUESE\": \"White\",\n",
    "        \"ASIAN - SOUTH EAST ASIAN\": \"Asian\",\n",
    "        \"WHITE - EASTERN EUROPEAN\": \"White\",\n",
    "        \"HISPANIC/LATINO - GUATEMALAN\": \"Hispanic/Latino\",\n",
    "        \"ASIAN - ASIAN INDIAN\": \"Asian\",\n",
    "        \"AMERICAN INDIAN/ALASKA NATIVE\": \"AIAN\",\n",
    "        \"HISPANIC OR LATINO\": \"Hispanic/Latino\",\n",
    "        \"WHITE - BRAZILIAN\": \"White\",\n",
    "        \"HISPANIC/LATINO - SALVADORAN\": \"Hispanic/Latino\",\n",
    "        \"HISPANIC/LATINO - COLUMBIAN\": \"Hispanic/Latino\",\n",
    "        \"HISPANIC/LATINO - HONDURAN\": \"Hispanic/Latino\",\n",
    "        \"HISPANIC/LATINO - CUBAN\": \"Hispanic/Latino\",\n",
    "        \"HISPANIC/LATINO - CENTRAL AMERICAN\": \"Hispanic/Latino\",\n",
    "        \"SOUTH AMERICAN\": \"Hispanic/Latino\",\n",
    "        \"UNABLE TO OBTAIN\": \"Unknown\",\n",
    "        \"HISPANIC/LATINO - MEXICAN\": \"Hispanic/Latino\",\n",
    "        \"ASIAN - KOREAN\": \"Asian\",\n",
    "        \"PATIENT DECLINED TO ANSWER\": \"Unknown\",\n",
    "        \"NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER\": \"NHPI\",\n",
    "        \"MULTIPLE RACE/ETHNICITY\": \"Other\",\n",
    "    }).fillna(\"Unknown\")\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_findings_tab1 = make_table_one(make_mimic_demo(dfs[\"mimic\"][\"findings\"]))\n",
    "mimic_impression_tab1 = make_table_one(make_mimic_demo(dfs[\"mimic\"][\"impression\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_findings_views = make_view_table(dfs[\"mimic\"][\"findings\"])\n",
    "mimic_impression_views = make_view_table(dfs[\"mimic\"][\"impression\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_findings_labels = make_label_table(dfs[\"mimic\"][\"findings\"])\n",
    "mimic_impression_labels = make_label_table(dfs[\"mimic\"][\"impression\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CheXpert Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chexpert_demo(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    demo = df.copy()\n",
    "\n",
    "    # combined race/ethnicity as per US OMB 2024 update\n",
    "    demo[\"Race\"] = demo[\"ethnicity\"].where(demo[\"ethnicity\"] == \"Hispanic/Latino\", other=demo[\"race\"])\n",
    "\n",
    "    demo[\"Race\"] = demo[\"Race\"].replace({\n",
    "        \"Pacific Islander\": \"NHPI\",\n",
    "        \"Native American\": \"AIAN\",\n",
    "        \"Patient Refused\": \"Unknown\",\n",
    "    })\n",
    "\n",
    "    demo[\"Sex\"] = demo[\"sex\"].copy()\n",
    "    demo[\"Age\"] = demo[\"age\"].copy()\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_findings_tab1 = make_table_one(make_chexpert_demo(dfs[\"chexpertplus\"][\"findings\"]))\n",
    "chexpert_impression_tab1 = make_table_one(make_chexpert_demo(dfs[\"chexpertplus\"][\"impression\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_findings_views = make_view_table(dfs[\"chexpertplus\"][\"findings\"])\n",
    "chexpert_impression_views = make_view_table(dfs[\"chexpertplus\"][\"impression\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_findings_labels = make_label_table(dfs[\"chexpertplus\"][\"findings\"])\n",
    "chexpert_impression_labels = make_label_table(dfs[\"chexpertplus\"][\"impression\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save TeX Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\"chexpert\", \"mimic\"]:\n",
    "    for section in [\"findings\", \"impression\"]:\n",
    "        for table in [\"tab1\", \"views\", \"labels\"]:\n",
    "            with open(f\"../figs/other/{table}-{dataset}-{section}.tex\", \"w\") as f:\n",
    "                df = eval(f\"{dataset}_{section}_{table}\") # lazy man's solution\n",
    "                f.write(mimic_findings_tab1.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
