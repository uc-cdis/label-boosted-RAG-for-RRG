{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multimodal Cancer Modeling in the Age of Foundation Model Embeddings\n",
    "### Steven Song\\*, Morgan Borjigin-Wang\\*, Irene R. Madejski, Robert L. Grossman\n",
    "\n",
    "\\* Equal contribution\n",
    "\n",
    "Read our paper here: https://arxiv.org/abs/2505.07683\n",
    "\n",
    "***\n",
    "\n",
    "The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference dataset in cancer through its harmonized genomics, clinical, and imaging data. Numerous prior studies have developed bespoke deep learning models over TCGA for tasks such as cancer survival prediction. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive feature embeddings agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized.\n",
    "\n",
    "* **We investigate the ability to train classical machine learning models over multimodal, zero-shot FM embeddings of cancer data.**\n",
    "* We demonstrate the ease and additive effect of multimodal fusion, outperforming unimodal models.\n",
    "* Overall, we propose a simple, modernized approach to multimodal cancer modeling using FM embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9316c1b",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/StevenSong/multimodal-cancer-modeling/refs/heads/main/overview.png\" alt=\"conceptual overview figure\" width=\"50%\"/>\n",
    "\n",
    "Conceptually, the proposed framework does late fusion of unimodal models trained over their respective embeddings. Specifically, we use:\n",
    "* BulkRNABert ([Gélard et al. 2025)](https://proceedings.mlr.press/v259/gelard25a.html)) for RNA-seq data\n",
    "* UNI2-h ([Chen et al. 2024](https://www.nature.com/articles/s41591-024-02857-3)) for histology data\n",
    "* BioMistral ([Labrak et al. 2024](https://aclanthology.org/2024.findings-acl.348/)) for pathology report data (summarized by Llama-3.1-8B-Instruct ([Grattafiori et al. 2024](https://arxiv.org/abs/2407.21783)))\n",
    "\n",
    "We can specifically break down the required steps for our approach as follows:\n",
    "1. Pull TCGA-LUAD embeddings\n",
    "1. Pull TCGA-LUAD metadata\n",
    "1. Merge embeddings and metadata\n",
    "1. Prepare experiments\n",
    "1. Train unimodal models\n",
    "1. Train multimodal model\n",
    "1. Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5py pandas numpy tqdm pqdm scikit-learn scikit-survival jupyterlab ipywidgets jsonrpcclient gen3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pqdm.threads import pqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from jsonrpcclient import request, parse_json, Ok, Error\n",
    "from gen3.auth import Gen3Auth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bade4b3",
   "metadata": {},
   "source": [
    "#### Pull TCGA Embeddings\n",
    "This retrieves embeddings using the AI commons embedding service!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ff80b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd044c773d2e47d7aa346425dc16019c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/439 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892da221cb5e4c1f895978971efa6a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/439 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070e6ac743724245897a11397454c9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/439 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439/439 [00:00<00:00, 11547.02it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8d36fd2ee4493da88c786805a314e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/433 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2966b257b1de492fbd98b7a047376f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/433 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fd4c40453345c6937dc58e93dd6d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/433 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 433/433 [00:00<00:00, 3080.44it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546787efd4df469985939d74ceccfa13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10b453b80614be381cfb84e49572dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5ca071a6704510a0f98422ce598233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:00<00:00, 1152.36it/s]\n"
     ]
    }
   ],
   "source": [
    "auth = Gen3Auth()\n",
    "\n",
    "expr_embs = dict()\n",
    "hist_embs = dict()\n",
    "text_embs = dict()\n",
    "\n",
    "def query(guid):\n",
    "    response = requests.get(\n",
    "        f\"https://m3aicommons.org/user/data/download/{guid}\",\n",
    "        auth=auth,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"url\"]\n",
    "\n",
    "for fpath, embs, do_upper in [\n",
    "    (\"manifests/expr-luad.json\", expr_embs, False),\n",
    "    (\"manifests/hist-luad.json\", hist_embs, False),\n",
    "    (\"manifests/summ-luad.json\", text_embs, True),\n",
    "]:\n",
    "    with open(fpath, \"r\") as f:\n",
    "        guids = [x[\"guid\"] for x in json.load(f)]\n",
    "    temp = pqdm(guids, query, n_jobs=500) # ongoing work to batch download\n",
    "    for x in tqdm(temp):\n",
    "        fid = x[\"file_id\"]\n",
    "        if do_upper: # some modalities need uppercased keys, artifact of upstream data preprocessing (from other research groups) mistmatch\n",
    "            fid = fid.upper()\n",
    "        embs[fid] = np.asarray(json.loads(x[\"embedding\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13836240",
   "metadata": {},
   "source": [
    "#### Pull TCGA Metadata\n",
    "\n",
    "In our embedding service, the only metadata we store with the embeddings is the filename. This is done to prevent extensive duplication of metadata columns from the GDC. We can use the GDC API itself to get a mapping of the filenames to other metadata, such as case ID and survival outcomes.\n",
    "\n",
    "Here, we also demo using the GDC Cohort Copilot as an MCP tool to construct GDC cohort filters. Ideally this would be done using an LLM, however we can also manually use the tool! This allows us to more easily construct the aforementioned metadata mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfcc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://apps.m3aicommons.org/gdc-cohort-copilot/gradio_api/mcp/\"\n",
    "headers = {\"Accept\": \"application/json, text/event-stream\"}\n",
    "\n",
    "# helper to parse the SSE-formatted response from the MCP server\n",
    "def parse_sse(response):\n",
    "    jsonrpc_str = response.text.split(\"\\n\", 1)[1].replace(\"data: \", \"\", 1)\n",
    "    match parse_json(jsonrpc_str):\n",
    "        case Ok(result, id_):\n",
    "            return result\n",
    "        case Error(code, message, data, id_):\n",
    "            raise ValueError(message)\n",
    "\n",
    "# helper to use the MCP tool to construct a GDC cohort filter\n",
    "def get_cohort_filter(query: str) -> str:\n",
    "    response = requests.post(\n",
    "        endpoint,\n",
    "        json=request(\n",
    "            method=\"tools/call\",\n",
    "            params={\n",
    "                \"name\": \"generate_filter\",\n",
    "                \"arguments\": {\n",
    "                    \"query\": query,\n",
    "                },\n",
    "            },\n",
    "        ),\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "    result = parse_sse(response)\n",
    "    return result[\"content\"][0][\"text\"]\n",
    "\n",
    "# helper to convert GDC request to file mapping\n",
    "def get_file_mapping(cohort_filter):\n",
    "    df = pd.read_csv(\n",
    "        StringIO(\n",
    "            requests.get(\n",
    "                \"https://api.gdc.cancer.gov/files\",\n",
    "                params={\n",
    "                    \"filters\": cohort_filter,\n",
    "                    \"fields\": \",\".join(\n",
    "                        [\n",
    "                            \"file_name\",\n",
    "                            \"cases.project.project_id\",\n",
    "                            \"cases.submitter_id\",\n",
    "                            \"experimental_strategy\",\n",
    "                            \"file_size\",\n",
    "                            \"md5sum\",\n",
    "                            \"state\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    \"format\": \"TSV\",\n",
    "                    \"size\": str(100_000),\n",
    "                },\n",
    "            ).text\n",
    "        ),\n",
    "        sep=\"\\t\",\n",
    "    )\n",
    "    mapping = df[[\"cases.0.submitter_id\", \"file_name\"]].rename(\n",
    "        columns={\n",
    "            \"cases.0.submitter_id\": \"case_id\",\n",
    "        }\n",
    "    )\n",
    "    mapping[\"file_name\"] = (\n",
    "        mapping[\"file_name\"]\n",
    "        .str.replace(\".svs\", \"\")\n",
    "        .str.replace(\".rna_seq.augmented_star_gene_counts.tsv\", \"\")\n",
    "        .str.replace(\".PDF\", \"\")\n",
    "    )\n",
    "    return mapping.set_index(\"case_id\")[\"file_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59747a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tools\": [\n",
      "        {\n",
      "            \"name\": \"generate_filter\",\n",
      "            \"description\": \"Converts a free text description of a cancer cohort into a GDC structured cohort filter. Returns: str: JSON structured GDC cohort filter\",\n",
      "            \"inputSchema\": {\n",
      "                \"type\": \"object\",\n",
      "                \"properties\": {\n",
      "                    \"query\": {\n",
      "                        \"type\": \"string\",\n",
      "                        \"description\": \"The free text cohort description\"\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get the tool list\n",
    "response = requests.post(\n",
    "    endpoint,\n",
    "    json=request(method=\"tools/list\"),\n",
    "    headers=headers,\n",
    ")\n",
    "\n",
    "print(json.dumps(parse_sse(response), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f697579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some cohort filters, you can print these to see what they look like!\n",
    "hist_filter = get_cohort_filter(\"TCGA-LUAD samples with diagnostic slides as svs files\")\n",
    "expr_filter = get_cohort_filter(\"TCGA-LUAD samples with RNA-sequences gene expression quantification data as tsv files\")\n",
    "text_filter = get_cohort_filter(\"TCGA-LUAD samples with pathology report pdf files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519d8247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"op\": \"and\",\n",
      "    \"content\": [\n",
      "        {\n",
      "            \"op\": \"in\",\n",
      "            \"content\": {\n",
      "                \"field\": \"cases.project.project_id\",\n",
      "                \"value\": [\n",
      "                    \"TCGA-LUAD\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"op\": \"in\",\n",
      "            \"content\": {\n",
      "                \"field\": \"files.experimental_strategy\",\n",
      "                \"value\": [\n",
      "                    \"Diagnostic Slide\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"op\": \"in\",\n",
      "            \"content\": {\n",
      "                \"field\": \"files.data_format\",\n",
      "                \"value\": [\n",
      "                    \"svs\"\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"op\": \"and\",\n",
      "    \"content\": [\n",
      "        {\n",
      "            \"op\": \"in\",\n",
      "            \"content\": {\n",
      "                \"field\": \"cases.project.project_id\",\n",
      "                \"value\": [\n",
      "                    \"TCGA-LUAD\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"op\": \"in\",\n",
      "            \"content\": {\n",
      "                \"field\": \"files.data_type\",\n",
      "                \"value\": [\n",
      "                    \"Gene Expression Quantification\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"op\": \"in\",\n",
      "            \"content\": {\n",
      "                \"field\": \"files.experimental_strategy\",\n",
      "                \"value\": [\n",
      "                    \"RNA-Seq\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"op\": \"in\",\n",
      "            \"content\": {\n",
      "                \"field\": \"files.data_format\",\n",
      "                \"value\": [\n",
      "                    \"tsv\"\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"op\": \"and\",\n",
      "    \"content\": [\n",
      "        {\n",
      "            \"op\": \"in\",\n",
      "            \"content\": {\n",
      "                \"field\": \"cases.project.project_id\",\n",
      "                \"value\": [\n",
      "                    \"TCGA-LUAD\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"op\": \"in\",\n",
      "            \"content\": {\n",
      "                \"field\": \"cases.project.program.name\",\n",
      "                \"value\": [\n",
      "                    \"TCGA\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"op\": \"in\",\n",
      "            \"content\": {\n",
      "                \"field\": \"files.data_type\",\n",
      "                \"value\": [\n",
      "                    \"Pathology Report\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"op\": \"in\",\n",
      "            \"content\": {\n",
      "                \"field\": \"files.data_format\",\n",
      "                \"value\": [\n",
      "                    \"pdf\"\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(hist_filter)\n",
    "print(expr_filter)\n",
    "print(text_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c6180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the API using the filters to get a mapping of case ID --> filename (from the embedding service)\n",
    "hist_mapping = get_file_mapping(hist_filter)\n",
    "expr_mapping = get_file_mapping(expr_filter)\n",
    "text_mapping = get_file_mapping(text_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18960731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clinical metadata, this is standard code to pull data from the GDC\n",
    "# (though we use an extra call to the GDC Cohort Copilot for fun)\n",
    "response = requests.get(\n",
    "    \"https://api.gdc.cancer.gov/cases\",\n",
    "    params={\n",
    "        \"filters\": get_cohort_filter(\"samples from TCGA-LUAD\"),\n",
    "        \"fields\": \",\".join(\n",
    "            [\n",
    "                \"project.project_id\",\n",
    "                \"submitter_id\",\n",
    "                \"diagnoses.age_at_diagnosis\",\n",
    "                \"diagnoses.diagnosis_is_primary_disease\",\n",
    "                \"diagnoses.days_to_last_follow_up\",\n",
    "                \"demographic.days_to_death\",\n",
    "                \"demographic.vital_status\",\n",
    "                \"demographic.ethnicity\",\n",
    "                \"demographic.gender\",\n",
    "                \"demographic.race\",\n",
    "            ]\n",
    "        ),\n",
    "        \"format\": \"JSON\",\n",
    "        \"size\": str(100_000),\n",
    "    },\n",
    ")\n",
    "temp = json.loads(response.content)\n",
    "\n",
    "data = []\n",
    "for sample in temp[\"data\"][\"hits\"]:\n",
    "    if \"diagnoses\" not in sample:\n",
    "        continue\n",
    "    for diagnosis in sample[\"diagnoses\"]:\n",
    "        if \"diagnosis_is_primary_disease\" in diagnosis and diagnosis[\"diagnosis_is_primary_disease\"]:\n",
    "            break\n",
    "    else:\n",
    "        # no primary disease, skip\n",
    "        continue\n",
    "    if \"days_to_last_follow_up\" not in diagnosis:\n",
    "        # primary diagnosis does not have f/u\n",
    "        continue\n",
    "    data.append({\n",
    "        \"project_id\": sample[\"project\"][\"project_id\"],\n",
    "        \"case_id\": sample[\"submitter_id\"],\n",
    "        \"age_at_diagnosis\": diagnosis[\"age_at_diagnosis\"],\n",
    "        \"days_to_last_fu\": diagnosis[\"days_to_last_follow_up\"],\n",
    "        \"sex\": sample[\"demographic\"][\"gender\"],\n",
    "        \"race\": sample[\"demographic\"][\"race\"],\n",
    "        \"ethnicity\": sample[\"demographic\"][\"ethnicity\"],\n",
    "        \"vital_status\": sample[\"demographic\"][\"vital_status\"],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2dc79",
   "metadata": {},
   "source": [
    "#### Merge Embeddings and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae751c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ids = ( # case IDs that have all 3 embedding modalities\n",
    "    set(expr_mapping[expr_mapping.isin(expr_embs)].index) &\n",
    "    set(hist_mapping[hist_mapping.isin(hist_embs)].index) &\n",
    "    set(text_mapping[text_mapping.isin(text_embs)].index)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d54ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df = df[\n",
    "    df[\"case_id\"].isin(case_ids)\n",
    "    & df[\"vital_status\"].isin({\"Alive\", \"Dead\"})\n",
    "    & df[\"age_at_diagnosis\"].notna()\n",
    "    & df[\"days_to_last_fu\"].notna()\n",
    "    & (df[\"days_to_last_fu\"] >= 0)\n",
    "]\n",
    "df[\"age_at_diagnosis\"] /= 365.24 # convert age to years\n",
    "df[\"age_binned\"] = pd.cut(df[\"age_at_diagnosis\"], bins=np.arange(0, 101, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0386fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_X = []\n",
    "hist_X = []\n",
    "text_X = []\n",
    "for _, row in df.iterrows():\n",
    "    case_id = row[\"case_id\"]\n",
    "    for embs, mapping, X in [\n",
    "        (expr_embs, expr_mapping, expr_X),\n",
    "        (hist_embs, hist_mapping, hist_X),\n",
    "        (text_embs, text_mapping, text_X),\n",
    "    ]:\n",
    "        fnames = mapping.loc[case_id]\n",
    "        if isinstance(fnames, str):\n",
    "            emb = embs[fnames] # fname is guaranteed to be in embeddings\n",
    "        else:\n",
    "            # at least one fname will be in embs\n",
    "            emb = np.mean([embs[f] for f in fnames if f in embs], axis=0)\n",
    "        X.append(emb)\n",
    "expr_X = np.asarray(expr_X)\n",
    "hist_X = np.asarray(hist_X)\n",
    "text_X = np.asarray(text_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4dc26",
   "metadata": {},
   "source": [
    "#### Prepare Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_X = OneHotEncoder(drop=\"if_binary\", sparse_output=False, dtype=np.float32).fit_transform(df[[\"sex\", \"age_binned\", \"race\", \"ethnicity\"]])\n",
    "# canc_X = OneHotEncoder(drop=\"if_binary\", sparse_output=False, dtype=np.float32).fit_transform(df[[\"project_id\"]]) # cancer type doesn't change for single project demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0aa4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    list(zip(df[\"vital_status\"] == \"Dead\", df[\"days_to_last_fu\"])),\n",
    "    dtype=[(\"Status\", \"?\"), (\"Survival_in_days\", \"<f8\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "030d0f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ( # stratify by observed mortality and cancer type\n",
    "    df[\"vital_status\"]\n",
    "    + df[\"project_id\"]\n",
    ")\n",
    "\n",
    "( # split all data modalities into train/test\n",
    "    demo_X_train,\n",
    "    demo_X_test,\n",
    "    # canc_X_train,\n",
    "    # canc_X_test,\n",
    "    expr_X_train,\n",
    "    expr_X_test,\n",
    "    hist_X_train,\n",
    "    hist_X_test,\n",
    "    text_X_train,\n",
    "    text_X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    ") = train_test_split(\n",
    "    demo_X,\n",
    "    # canc_X,\n",
    "    expr_X,\n",
    "    hist_X,\n",
    "    text_X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "814a16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one helper will be used to run both unimodal and multimodal experiments\n",
    "def train_eval_model(\n",
    "    *,  # enforce kwargs\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    pca_components: int | None,\n",
    "    standardize: bool,\n",
    "    name: str = \"\",\n",
    "    verbose: bool = False,\n",
    ") -> dict:\n",
    "    if verbose:\n",
    "        print(f\"Running {name}\")\n",
    "\n",
    "    # z-score input features\n",
    "    if standardize:\n",
    "        if verbose:\n",
    "            print(\"--standardized\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    # dimensionality reduction\n",
    "    if pca_components is not None:\n",
    "        if verbose:\n",
    "            print(\"--reduced\")\n",
    "        pca = PCA(n_components=pca_components, random_state=42)\n",
    "        X_train_red = pca.fit_transform(X_train_scaled)\n",
    "        X_test_red = pca.transform(X_test_scaled)\n",
    "    else:\n",
    "        X_train_red = X_train_scaled\n",
    "        X_test_red = X_test_scaled\n",
    "\n",
    "    # fit survival model\n",
    "    cox = CoxPHSurvivalAnalysis(alpha=0.1).fit(X_train_red, y_train)\n",
    "    if verbose:\n",
    "        print(\"--trained\")\n",
    "\n",
    "    # generate predictions\n",
    "    y_train_pred = cox.predict(X_train_red)\n",
    "    y_test_pred = cox.predict(X_test_red)\n",
    "\n",
    "    # evaluate predictions\n",
    "    c_index = concordance_index_censored(\n",
    "        event_indicator=y_test[\"Status\"],\n",
    "        event_time=y_test[\"Survival_in_days\"],\n",
    "        estimate=y_test_pred,\n",
    "    )[0]\n",
    "\n",
    "    return {\n",
    "        \"c_index\": c_index,\n",
    "        \"y_test_pred\": y_test_pred,\n",
    "        \"y_train_pred\": y_train_pred,\n",
    "        \"coxph\": cox,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b0aad1",
   "metadata": {},
   "source": [
    "#### Train Unimodal Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c04c6047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running demo\n",
      "--trained\n",
      "Running expr\n",
      "--standardized\n",
      "--reduced\n",
      "--trained\n",
      "Running hist\n",
      "--standardized\n",
      "--reduced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/gpudata/steven/miniforge3/envs/demo/lib/python3.12/site-packages/sksurv/linear_model/coxph.py:200: RuntimeWarning: overflow encountered in exp\n",
      "  risk_set += np.exp(xw[k])\n",
      "/opt/gpudata/steven/miniforge3/envs/demo/lib/python3.12/site-packages/sksurv/linear_model/coxph.py:197: RuntimeWarning: overflow encountered in exp\n",
      "  risk_set2 += np.exp(xw[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--trained\n",
      "Running text\n",
      "--standardized\n",
      "--reduced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/gpudata/steven/miniforge3/envs/demo/lib/python3.12/site-packages/sksurv/linear_model/coxph.py:200: RuntimeWarning: overflow encountered in exp\n",
      "  risk_set += np.exp(xw[k])\n",
      "/opt/gpudata/steven/miniforge3/envs/demo/lib/python3.12/site-packages/sksurv/linear_model/coxph.py:197: RuntimeWarning: overflow encountered in exp\n",
      "  risk_set2 += np.exp(xw[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--trained\n"
     ]
    }
   ],
   "source": [
    "demo_results = train_eval_model(X_train=demo_X_train, y_train=y_train, X_test=demo_X_test, y_test=y_test, pca_components=None, standardize=False, name=\"demo\", verbose=True)\n",
    "# canc_results = train_eval_model(X_train=canc_X_train, y_train=y_train, X_test=canc_X_test, y_test=y_test, pca_components=None, standardize=False, name=\"canc\", verbose=True)\n",
    "expr_results = train_eval_model(X_train=expr_X_train, y_train=y_train, X_test=expr_X_test, y_test=y_test, pca_components=256, standardize=True, name=\"expr\", verbose=True)\n",
    "hist_results = train_eval_model(X_train=hist_X_train, y_train=y_train, X_test=hist_X_test, y_test=y_test, pca_components=256, standardize=True, name=\"hist\", verbose=True)\n",
    "text_results = train_eval_model(X_train=text_X_train, y_train=y_train, X_test=text_X_test, y_test=y_test, pca_components=256, standardize=True, name=\"text\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c978bad",
   "metadata": {},
   "source": [
    "#### Train Multimodal Model\n",
    "The unimodal models' predicted risk scores are used as input to the multimodal fusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daf8c3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fuse\n",
      "--standardized\n",
      "--trained\n"
     ]
    }
   ],
   "source": [
    "fuse_X_train = np.asarray([\n",
    "    demo_results[\"y_train_pred\"],\n",
    "    # canc_results[\"y_train_pred\"],\n",
    "    expr_results[\"y_train_pred\"],\n",
    "    hist_results[\"y_train_pred\"],\n",
    "    text_results[\"y_train_pred\"],\n",
    "]).T\n",
    "\n",
    "fuse_X_test = np.asarray([\n",
    "    demo_results[\"y_test_pred\"],\n",
    "    # canc_results[\"y_test_pred\"],\n",
    "    expr_results[\"y_test_pred\"],\n",
    "    hist_results[\"y_test_pred\"],\n",
    "    text_results[\"y_test_pred\"],\n",
    "]).T\n",
    "\n",
    "fuse_results = train_eval_model(X_train=fuse_X_train, y_train=y_train, X_test=fuse_X_test, y_test=y_test, pca_components=None, standardize=True, name=\"fuse\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2417d",
   "metadata": {},
   "source": [
    "#### Evaluate Models\n",
    "Our multimodal fusion approach substantially beats all unimodal results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "560edb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unimodal Results\n",
      "------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "demo    0.578692\n",
       "expr    0.619855\n",
       "hist    0.654560\n",
       "text    0.686037\n",
       "Name: C-index, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal Results\n",
      "------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fuse    0.736885\n",
       "Name: C-index, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.Series({\n",
    "    \"demo\": demo_results[\"c_index\"],\n",
    "    # \"canc\": canc_results[\"c_index\"],\n",
    "    \"expr\": expr_results[\"c_index\"],\n",
    "    \"hist\": hist_results[\"c_index\"],\n",
    "    \"text\": text_results[\"c_index\"],\n",
    "    \"fuse\": fuse_results[\"c_index\"],\n",
    "}, name=\"C-index\")\n",
    "\n",
    "print(\"Unimodal Results\")\n",
    "print(\"------------------\")\n",
    "display(results.loc[[\"demo\", \"expr\", \"hist\", \"text\"]])\n",
    "\n",
    "print(\"Multimodal Results\")\n",
    "print(\"------------------\")\n",
    "display(results.loc[[\"fuse\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd006e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
